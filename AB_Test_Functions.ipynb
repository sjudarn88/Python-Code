{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658aa294-6cea-486a-b848-85cf764d2a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import Packages & Define Functions\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.types as T\n",
    "from statsmodels.stats.proportion import proportions_ztest, proportion_confint\n",
    "from pyspark.sql.types import LongType,DoubleType, FloatType, ArrayType,IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68d08ae-98da-4b81-baf7-ae50007dd8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Z-test\n",
    "def parse_array_from_string(x):\n",
    "\tres = json.loads(x)\n",
    "\treturn res\n",
    "  \n",
    "retrieve_array = udf(parse_array_from_string, T.ArrayType(T.StringType()))\n",
    "\n",
    "# Define statistics\n",
    "def ztest(control_count, treatment_count, control_success, treatment_success,tail='larger',threshold=0.05,center=0):\n",
    "  \"\"\"\n",
    "    tail: \"two-sided\" is for double-tailed test. \"larger\" is for right-tailed test. \"smaller is for left-tailed test\"\n",
    "    threshold: alpha, ususally 0.01,0.05,0.1.azure\n",
    "    center: 0 if comparing mean of the sample, a number is how far away is added/substracted from mean of the sample. \n",
    "  \"\"\"\n",
    "  success = [control_success, treatment_success]\n",
    "  nr_obs = [control_count, treatment_count]\n",
    "  #pvalue\n",
    "  z_stat,pvalue = proportions_ztest(count=success,nobs=nr_obs,value=center,alternative=tail)\n",
    "  #confidence interval\n",
    "  (lower_control,lower_treatment),(upper_control,upper_treatment) = proportion_confint(success,nobs=nr_obs,alpha=threshold)\n",
    "  return (float(z_stat),float(pvalue),float(lower_control),float(lower_treatment),float(upper_control),float(upper_treatment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5f3720-77f5-42f5-ad8a-cef10453f399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Output Table for Ztest\n",
    "def create_result(tb):\n",
    "  ptb = tb.filter(col('GROUP')=='control').alias('c').crossJoin(tb.filter(col('GROUP')=='treatment').alias('t'))\\\n",
    "            .select(col(\"c.COUNT\").alias(\"control_count\"),col(\"t.COUNT\").alias(\"treatment_count\"), \n",
    "              col(\"c.SUCCESS\").alias(\"control_success\"),col(\"t.SUCCESS\").alias(\"treatment_success\"))\n",
    "  \n",
    "  ZtestUDF = udf(ztest,ArrayType(DoubleType()))\n",
    "  result_df = ptb.withColumn('ztest_udf',ZtestUDF('control_count','treatment_count','control_success','treatment_success'))\n",
    "  result_df = result_df.withColumn('control_CR',round(col(\"control_success\")/col(\"control_count\"),4))\\\n",
    "                     .withColumn('treatment_CR',round(col(\"treatment_success\")/col(\"treatment_count\"),4))\\\n",
    "                     .withColumn('z_stat',round(result_df.ztest_udf[0],2))\\\n",
    "                     .withColumn('p_value',round(result_df.ztest_udf[1],4))\\\n",
    "                     .withColumn('control CI',array(round(result_df.ztest_udf[2],4),round(result_df.ztest_udf[4],4)))\\\n",
    "                     .withColumn('treatment CI',array(round(result_df.ztest_udf[3],4),round(result_df.ztest_udf[5],4)))\n",
    "  result_df = result_df.drop(result_df.ztest_udf)\n",
    "\n",
    "  return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c61ffb-eb8f-4ce0-a38e-b6980285f912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['e_score', 'e2_score', 'ec_score', 'bt_score', 'lc_score', 'ln_score', 'fico'])\n"
     ]
    }
   ],
   "source": [
    "### Model Score Adjustment Across Underwriting Models\n",
    "\n",
    "#list of different underwriting models\n",
    "loss_odds_dict = {\n",
    "    'e_score' : {\n",
    "\t\t 'f': lambda x,c: 1/(1+np.exp(-c[0]*(x-c[1]))),\n",
    "\t\t 'c':[3.7921, 1.2382]\n",
    "\t} ,\n",
    "    'e2_score' : {\n",
    "\t\t 'f': lambda x,c: 1/(1+np.exp(-c[0]*(x-c[1]))),\n",
    "\t\t 'c':[5.4512, 1.1943]\n",
    "\t} ,\n",
    "\t'ec_score' : {\n",
    "\t\t 'f': lambda x,c: c[0]/(1+np.exp(-c[1]*(x-c[2]))) + c[3],\n",
    "\t\t 'c':[0.1097, 80.02746, 0.0416, 0]\n",
    "\t} ,\n",
    "\t'bt_score' : {\n",
    "\t\t 'f': lambda x,c: c[0]/(1+np.exp(-c[1]*(x-c[2]))) + c[3],\n",
    "\t\t 'c':[0.1662, 6.8883, 0.5749, 0]\n",
    "\t} ,\n",
    "\t'lc_score' : {\n",
    "\t\t 'f': lambda x,c: c[0]/(1+np.exp(-c[1]*(x-c[2]))) + c[3],\n",
    "\t\t 'c':[2, 10.6703, 1.10740, 0.0034]\n",
    "\t} ,\n",
    "\t'ln_score' : {\n",
    "\t\t 'f': lambda x,c: c[0]/(1+np.exp(-c[1]*(x-c[2]))) + c[3],\n",
    "\t\t 'c':[2, 5.0349, 1.3965, 0.0154]\n",
    "\t} ,\n",
    "\t'fico' : {\n",
    "\t\t 'f': lambda x,c: c[0]*np.sqrt(c[2]*(x-c[3])**2+c[1])/(x-c[3]) - c[0],\n",
    "\t\t 'c':[-62.7958, -79.1611, 1.0005, 405.1482]\n",
    "\t} ,\n",
    " \n",
    "}\n",
    "\n",
    "print(loss_odds_dict.keys())\n",
    "\n",
    "def get_loss_odds(model_name, score, default_value = np.nan):\n",
    "    \"\"\" Computes loss odds from a model score usinging an analytical fit function. \n",
    "    Possible models at this time are: e_score, ec_score, bt_score, lc_score,\n",
    "    ln_score, and fico.\n",
    "\n",
    "    Returns etimated loss odds.\n",
    "\n",
    "    :param model_name: name of model\n",
    "    :type model_name: str\n",
    "    :param model_score: model score\n",
    "    :type model_name: float\n",
    "    :param default_value: value that will be imputed if model_name is missing\n",
    "    \n",
    "    :return: The odds of a loan defaulting\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    if model_name in loss_odds_dict.keys():\n",
    "        return LOSS_DICT[model_name]['f'](score, loss_odds_dict[model_name]['c'])\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "def get_df_loss_odds(row, default_val=np.nan):\n",
    "    \"\"\" Computes loss odds from analytical fit function for a model score from\n",
    "    a pandas dataframe. Use: df['loss_odds'] = df.apply(get_df_loss_odds, axis=1) \n",
    "    if the columns for 'model_used' and 'model_score' are in the dataframe.\n",
    "    Possible models at this time are: e_score, ec_score, bt_score, lc_score,\n",
    "     ln_score, and fico.\n",
    "\n",
    "    Returns etimated loss odds.\n",
    "\n",
    "    :param row:  Dictionary or pandas Series with 'model_score' or 'fico' keys.\n",
    "    :type row: dict{str:float}\n",
    "    :param default_value: value that will be imputed if model_name is missing\n",
    "    :type model_name: float\n",
    "   \n",
    "    :return: The odds of a loan defaulting\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    if row['model_used'] in loss_odds_dict.keys():\n",
    "        return loss_odds_dict(row['model_used'], row['model_score'])\n",
    "    elif row['fico'].notna():\n",
    "        return loss_odds_dict('fico', row['fico'])\n",
    "    else:\n",
    "        return default_val\n",
    "      \n",
    "loss_odds = udf(get_loss_odds, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34009e0d-0097-407e-a563-b817335af8f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Function that returns stratified data after sampling on the original dataframe.\n",
    "def stratify_data(df_control,df_treatment,param,col_name,random_state=50):\n",
    "  \"\"\"\n",
    "  create strata and proportions of each strata\n",
    "  \"\"\"\n",
    "  level_tr = df_treatment.groupBy(col_name).count()\n",
    "  level_tr = level_tr.withColumn('PCT',col('count')/sum('count').over(Window.partitionBy()))\\\n",
    "                     .withColumn('NEW_COL',concat(*col_name))  \n",
    "  strata = level_tr.rdd.map(lambda x: x.NEW_COL).collect()\n",
    "  proportions = level_tr.rdd.map(lambda x: x.PCT).collect()\n",
    "  display(level_tr.select('NEW_COL','count','PCT').orderBy('NEW_COL'))\n",
    "\n",
    "  \"\"\"Stratifies data according to the values and proportions passed in\n",
    "  Args: \n",
    "      df_control or df_treatment (DataFrame): source data\n",
    "      col_name (str): The name of the single column in the dataframe that holds the data values that will be used to stratify the data\n",
    "      strata (list of str): A list of all of the potential values for stratifying e.g. \"Male, Graduate\", \"Male, Undergraduate\", \"Female, Graduate\", \"Female, Undergraduate\"\n",
    "      proportions (list of float): A list of numbers representing the desired propotions for stratifying e.g. 0.4, 0.4, 0.2, 0.2, The list values must add up to 1 and must match the number of values in stratify_values\n",
    "      random_state (int, optional): sets the random_state. Defaults to None.\n",
    "  Returns:\n",
    "      DataFrame: a new dataframe based on df_data that has the new proportions represnting the desired strategy for stratifying\n",
    "  \"\"\"\n",
    "  df_control = df_control.withColumn('NEW_COL',concat(*col_name))\n",
    "\n",
    "  spark = SparkSession.builder.appName('Empty_Dataframe').getOrCreate()\n",
    "  emp_RDD = spark.sparkContext.emptyRDD()\n",
    "  columns = df_control.schema\n",
    "  df_stratified = spark.createDataFrame(data = emp_RDD,schema = columns)\n",
    "\n",
    "  pos = -1\n",
    "  #for i in range(2):\n",
    "  for i in range(len(strata)):\n",
    "    pos += 1\n",
    "    if pos == len(strata) - 1:\n",
    "      ratio_len = int(df_control.count() * param) - df_stratified.count()\n",
    "    else:\n",
    "      ratio_len = int(df_control.count() * param * proportions[i])\n",
    "\n",
    "    df_filtered = df_control[df_control['NEW_COL'] == strata[i]]\n",
    "    temp = df_filtered.rdd.takeSample(True, ratio_len, random_state) \n",
    "    df_temp = spark.createDataFrame(data=temp, schema = df_control.schema)\n",
    "    df_stratified = df_stratified.unionAll(df_temp)\n",
    "  \n",
    "  #display(df_stratified.groupBy('NEW_COL').count().withColumn('PCT',col('count')/sum('count').over(Window.partitionBy())).orderBy('NEW_COL'))\n",
    "  return df_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149cefd6-1f99-4b6d-b239-cfdb3eb3ecdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bucketizer(bins,col_input,col_output,df,user_seg, p):\n",
    "  bucketizer = Bucketizer(splits = bins,inputCol = col_input, outputCol = col_output)\n",
    "\n",
    "  df_buck = bucketizer.setHandleInvalid(\"keep\").transform(df).na.drop(subset=[col_output])\n",
    "\n",
    "  df_buck_agg = df_buck.groupBy('CID','GROUP','USER_FLAG_FIRST',col_output)\\\n",
    "                  .agg(sum('IF_APPROVE').alias('IF_APPROVE'),sum('IF_ACCEPT').alias('IF_ACCEPT'))\\\n",
    "                  .withColumn('IF_APPROVE',when(col('IF_APPROVE') >= 1, lit('1')).otherwise(col('IF_APPROVE')))\\\n",
    "                  .withColumn('IF_ACCEPT',when(col('IF_ACCEPT') >= 1, lit('1')).otherwise(col('IF_ACCEPT')))\n",
    "  \n",
    "  df_tr = df_buck_agg.filter((col('GROUP')=='treatment'))\n",
    "  df_cr = df_buck_agg.filter((col('GROUP')=='control'))\n",
    "  \n",
    "  col_t = [col_output]\n",
    "  param = p\n",
    "  df_output = stratify_data(df_tr,df_cr,param,col_t)\n",
    "\n",
    "  tb_strata = df_cr.unionAll(df_output.drop('NEW_COL'))\n",
    "  #tb_strata = df_output_cr.unionAll(df_output_tr)\n",
    "\n",
    "  tb_strata_agg = tb_strata.groupBy('GROUP').agg(sum('IF_APPROVE').alias('COUNT'),sum('IF_ACCEPT').alias('SUCCESS'))\\\n",
    "            .withColumn('CR',round(col('SUCCESS')/col('COUNT'),4))\n",
    "\n",
    "  df_strata = create_result(tb_strata_agg).withColumn('user segment',lit(user_seg))\n",
    "  return (df_strata,tb_strata)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "AB_Test_Functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}